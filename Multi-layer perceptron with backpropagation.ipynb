{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ead190ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f6112",
   "metadata": {},
   "source": [
    "The aim of the following functions is to create an artificial dataset to test multi-layer perceptron with backpropagation algorithm. Here I have made functions to create datasets for binary classification tasks and for regression tasks.\n",
    "\n",
    "The idea is to generate sets of k points (one for each class) at random and use the distance from these points to determine the class or the regression target. The precise way to integrate the distance information from k points into a single value is to return the product of the distance from the closest point times the distance from the furthest point.\n",
    "\n",
    "a) out = oracle(x, landmarks) which takes in input a vector x and a data matrix landmarks that contains a number of landmark vectors as rows. The function must consider only the closest and the furthest landmark to x and return the scalar value of the product of these two distances.\n",
    "\n",
    "b) (data_mtx, y) or (data_mtx, y, landmarks) = make_dataset_regression(size=100, complexity=2, ndim=3, return_landmarks=False) which outputs the data matrix data_mtx and the target vector y. The number of vectors in data_mtx is determined by the parameter size. The number of landmarks is determined by the parameter complexity. The number of dimensions of the landmark vectors and of the data matrix is determined by ndim. If the flag return_landmarks is set to True then the output of the function includes in addition the landmarks data matrix. The target vector y is computed using the function oracle. The vectors returned are sampled uniformly at random in the unit hyper-cube (i.e. the coordinates are between 0 and 1). The landmarks vectors are sampled uniformly at random in the unit hyper-cube.\n",
    "\n",
    "c) data_mtx, y = make_2d_grid_dataset_regression(size, landmarks) which outputs the data matrix data_mtx and the target vector y. The number of vectors in data_mtx is determined by the parameter size. The target vector y is computed using the function oracle using the landmarks passed in input in the data matrix landmarks. This function assumes that the dimension of the required vectors is 2 and generates a 2D grid of equally spaced vectors to fill the unit square (i.e. the coordinates are between 0 and 1). Note: if the size is 100 then the grid will be 10 x 10.\n",
    "\n",
    "d) y = oracle_classification(X, pos_landmarks, neg_landmarks) which takes in input a data matrix X and two data matrix pos_landmarks and neg_landmarks. The function will output a target vector containing the values 1 and 0: a vector x is associated to the target 1 when the value of the oracle function for the positive landmarks is less or equal to the value of the oracle function for the negative landmarks, 0 otherwise.\n",
    "\n",
    "e) (data_mtx, y) or (data_mtx, y, pos_landmarks, neg_landmarks) = make_dataset_classification(size=100, complexity=2, ndim=3, return_landmarks=False) which outputs the data matrix data_mtx and the target vector y. The number of vectors in data_mtx is determined by the parameter size. The number of landmarks is determined by the parameter complexity. The number of dimensions of the landmark vectors and of the data matrix is determined by ndim. If the flag return_landmarks is set to True then the output of the function includes in addition the positive and negative landmarks data matrices. The target vector y is computed using the function oracle_classification. The vectors returned are sampled uniformly at random in the unit hyper-cube (i.e. the coordinates are between 0 and 1). The landmarks vectors are sampled uniformly at random in the unit hyper-cube.\n",
    "\n",
    "f) data_mtx, y = make_2d_grid_dataset_classification(size, pos_landmarks, neg_landmarks) which outputs the data matrix data_mtx and the target vector y. The number of vectors in data_mtx is determined by the parameter size. The target vector y is computed using the function oracle_classification using the landmarks passed in input in the data matrices pos_landmarks and neg_landmarks. This function assumes that the dimension of the required vectors is 2 and generates a 2D grid of equally spaced vectors to fill the unit square (i.e. the coordinates are between 0 and 1). Note: if the size is 100 then the grid will be 10 x 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a776cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle(x, landmarks):\n",
    "    dist=np.zeros(np.shape(landmarks)[0])\n",
    "    for i in range(np.shape(landmarks)[0]):\n",
    "        dist[i]=np.linalg.norm(x-landmarks[i])\n",
    "    return dist[np.argsort(dist)[0]]*dist[np.argsort(dist)[-1]]\n",
    "\n",
    "def make_dataset_regression(size=100, complexity=2, ndim=3, return_landmarks=False):\n",
    "    landmarks=np.zeros([complexity,ndim],dtype=float)\n",
    "    for i in range(complexity):\n",
    "        for j in range(ndim):\n",
    "            landmarks[i][j]=np.random.uniform(low=0, high=1)\n",
    "    data_mtx=np.zeros([size,ndim],dtype=float)\n",
    "    y=np.zeros(size,dtype=float)\n",
    "    for i in range(size):\n",
    "        for j in range(ndim):\n",
    "            data_mtx[i][j]=np.random.uniform(low=0, high=1)\n",
    "        y[i]=oracle(data_mtx[i],landmarks)\n",
    "    if return_landmarks==True:\n",
    "        return data_mtx, y, landmarks\n",
    "    else:\n",
    "        return data_mtx, y\n",
    "    \n",
    "def make_2d_grid_dataset_regression(size, landmarks):\n",
    "    n=int(np.sqrt(size))\n",
    "    points=np.linspace(0,1,num=n)\n",
    "    data_mtx=np.array(np.meshgrid(points,points)).T.reshape(-1,2)\n",
    "    y=np.zeros(n*n,dtype=float)\n",
    "    for i in range(len(y)):\n",
    "        y[i]=oracle(data_mtx[i],landmarks)\n",
    "    return data_mtx,y\n",
    "    \n",
    "def oracle_classification(X, pos_landmarks, neg_landmarks):\n",
    "    y=np.zeros(np.shape(X)[0],dtype=float)\n",
    "    for i in range(np.shape(X)[0]):\n",
    "        p1=oracle(X[i],pos_landmarks)\n",
    "        p2=oracle(X[i],neg_landmarks)\n",
    "        if p1<=p2:\n",
    "            y[i]=1\n",
    "        else:\n",
    "            y[i]=0\n",
    "    return y\n",
    "\n",
    "def make_dataset_classification(size=100, complexity=2, ndim=3, return_landmarks=False):\n",
    "    pos_landmarks=np.zeros([complexity,ndim],dtype=float)\n",
    "    neg_landmarks=np.zeros([complexity,ndim],dtype=float)\n",
    "    for i in range(complexity):\n",
    "        for j in range(ndim):\n",
    "            pos_landmarks[i][j]=np.random.uniform(low=0, high=1)\n",
    "            neg_landmarks[i][j]=np.random.uniform(low=0, high=1)\n",
    "    data_mtx=np.zeros([size,ndim],dtype=float)\n",
    "    \n",
    "    for i in range(size):\n",
    "        for j in range(ndim):\n",
    "            data_mtx[i][j]=np.random.uniform(low=0, high=1)\n",
    "            \n",
    "    y=oracle_classification(data_mtx, pos_landmarks, neg_landmarks)\n",
    "    \n",
    "    if return_landmarks==True:\n",
    "        return data_mtx, y, pos_landmarks, neg_landmarks\n",
    "    else:\n",
    "        return data_mtx, y\n",
    "    \n",
    "def make_2d_grid_dataset_classification(size, pos_landmarks, neg_landmarks):\n",
    "    n=int(np.sqrt(size))\n",
    "    points=np.linspace(0,1,num=n,endpoint=False)\n",
    "    data_mtx=np.array(np.meshgrid(points,points)).T.reshape(-1,2)\n",
    "    y=oracle_classification(data_mtx, pos_landmarks, neg_landmarks)\n",
    "    \n",
    "    return data_mtx,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b4e45b",
   "metadata": {},
   "source": [
    "plot_2d_classification(X_test, y_test, preds, pos_landmarks, neg_landmarks) to display a data matrix X_test, colouring the instances according to their respective class. The data matrices pos_landmarks and neg_landmarks are also received in input to localize the landmarks. The function should plot three plots: the first where the instances in X_test are coloured using the target vector preds, the second using the target vector y_test and the third where the two coloring schemes are overlapped. To obtain the overlap effect consider using the color map gray for y_test with an alpha of 0.3. The last plot should report in the title the accuracy score of the predictions.\n",
    "\n",
    "plot_2d_regression(X_test, y_test, preds, landmarks) to display a data matrix X_test, colouring the instances according to their respective class. The data matrix landmarksis also received in input to localize the landmarks. The function should plot two plots: the first where the instances in X_test are coloured using the target vector preds, the second using the target vector y_test. The first plot should report in the title the correlation coefficient score of the predictions vs. the values in y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07631339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_classification(X_test, y_test, preds, pos_landmarks, neg_landmarks):\n",
    "    fig,ax=plt.subplots(nrows=1,ncols=3,figsize=(10,5))\n",
    "    x_coords=[]\n",
    "    y_coords=[]\n",
    "    for i in range(np.shape(X_test)[0]):\n",
    "        x_coords.append(X_test[i][0])\n",
    "        y_coords.append(X_test[i][1])\n",
    "        \n",
    "\n",
    "    ax[0].scatter(x_coords,y_coords,c=preds,cmap='bwr',alpha=0.2)\n",
    "    ax[0].set_title('Prediction')\n",
    "\n",
    "    for i in range(len(neg_landmarks)):\n",
    "        ax[0].plot(pos_landmarks[i][0],pos_landmarks[i][1],marker='d',markersize=10,color='blue')\n",
    "        ax[0].plot(neg_landmarks[i][0],neg_landmarks[i][1],marker='d',markersize=10,color='red')\n",
    "\n",
    "\n",
    "    ax[1].scatter(x_coords,y_coords,c=y_test,cmap='bwr',alpha=0.2)\n",
    "    ax[1].set_title('Truth')\n",
    "\n",
    "    accuracy=np.sum(preds==y_test)/len(preds)\n",
    "    for i in range(len(neg_landmarks)):\n",
    "        ax[1].plot(pos_landmarks[i][0],pos_landmarks[i][1],marker='d',markersize=10,color='blue')\n",
    "        ax[1].plot(neg_landmarks[i][0],neg_landmarks[i][1],marker='d',markersize=10,color='red')\n",
    "\n",
    "    ax[2].scatter(x_coords,y_coords,c=preds,cmap='bwr',alpha=0.3)\n",
    "    ax[2].scatter(x_coords,y_coords,c=y_test,cmap='bwr',alpha=0.1)\n",
    "    ax[2].set_title(f'Comparison Acc:{round(accuracy,3)}')\n",
    "\n",
    "    for i in range(len(neg_landmarks)):\n",
    "        ax[2].plot(pos_landmarks[i][0],pos_landmarks[i][1],marker='d',markersize=10,color='blue')\n",
    "        ax[2].plot(neg_landmarks[i][0],neg_landmarks[i][1],marker='d',markersize=10,color='red')\n",
    "    \n",
    "def plot_2d_regression(X_test, y_test, preds, landmarks):\n",
    "    fig,ax=plt.subplots(nrows=1,ncols=2,figsize=(10,5))\n",
    "    x_coords=[]\n",
    "    y_coords=[]\n",
    "\n",
    "    for i in range(np.shape(X_test)[0]):\n",
    "        x_coords.append(X_test[i][0])\n",
    "        y_coords.append(X_test[i][1])\n",
    "\n",
    "    correlation_mat=sp.stats.pearsonr(y_test,preds)\n",
    "    ax[0].scatter(x_coords,y_coords,c=preds,cmap='hot_r')\n",
    "    ax[0].set_title(f'Prediction cc {round(correlation_mat[0],3)}')\n",
    "    for i in range(len(landmarks)):\n",
    "        ax[0].plot(landmarks[i][0],landmarks[i][1],marker='d',markersize=10,color='blue')\n",
    "        \n",
    "\n",
    "    ax[1].scatter(x_coords,y_coords,c=y_test,cmap='hot_r')\n",
    "    ax[1].set_title('Truth')\n",
    "    for i in range(len(landmarks)):\n",
    "        ax[1].plot(landmarks[i][0],landmarks[i][1],marker='d',markersize=10,color='blue')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d369f",
   "metadata": {},
   "source": [
    "The aim of the following functions is to provide the implementations for the loss functions and the activation functions to be used in the generalized perceptron algorithm. In the following, the inputs y represent prediction(s), t the true value(s) and x the pre-activation value(s). These inputs can be scalars or vectors (both row or column vectors), that is these functions should work for inputs that are scalar or vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "042cbeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y, t):\n",
    "    MSE=np.square(np.subtract(t,y)).mean()\n",
    "    return MSE\n",
    "    \n",
    "def mse_loss_grad(y, t):\n",
    "    y=np.array(y)\n",
    "    t=np.array(t)\n",
    "    return 2*np.subtract(t,y)/y.size\n",
    "    \n",
    "def binary_crossentropy_loss(y, t):\n",
    "    t=np.array(t)\n",
    "    y=np.array(y)\n",
    "    eps=1e-10\n",
    "    return -np.mean((t*np.log(y+eps)+(1-t)+np.log(1-y+eps)))\n",
    "\n",
    "def binary_crossentropy_loss_grad(y, t):\n",
    "    eps=1e-10\n",
    "    return (t-y)/(t*(1-t)+eps)\n",
    "    \n",
    "def linear_activation(x):\n",
    "    return x\n",
    "    \n",
    "def linear_activation_grad(x):\n",
    "    if np.isscalar(x)==False:\n",
    "        dim=np.shape(x)\n",
    "        arr=np.ones(dim)\n",
    "        return arr\n",
    "    return 1\n",
    "    \n",
    "def logistic_activation(x):\n",
    "    \n",
    "    sigma=1/(1+np.exp(-x))\n",
    "    return sigma\n",
    "    \n",
    "def logistic_activation_grad(x):\n",
    "    s=logistic_activation(x)\n",
    "    s_grad=s*(1-s)\n",
    "    return s_grad\n",
    "    \n",
    "def relu_activation(x, alpha=.05):\n",
    "    return np.maximum(alpha*x,x)\n",
    "    \n",
    "def relu_activation_grad(x, alpha=.05):\n",
    "    \n",
    "    return np.where(x>0,1,alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1924652d",
   "metadata": {},
   "source": [
    "The aim of the following functions is to train artificial neural network type of classifiers and regressors. The classification is always intended as binary classification. Here I have written my own implementation for the cases of the multi layer perceptron with backpropagation rule. This implementation is more general and can accept in input arbitrary activation and loss functions for the units in the hidden layer and the output unit. The multi layer perceptron is always assumed to have a single output unit and a single hidden layer.\n",
    "\n",
    "Note here I have implemented a batch update, that is the model is not updated as it processes each instance, but rather the gradients are stored and averaged together whenever a number of instances equal to batch_size has been observed.\n",
    "\n",
    "In the following perceptron_model = (W_ih, b_ih, W_ho, b_ho, hidden_activation_func, hidden_activation_func_grad, out_activation_func, out_activation_func_grad, loss, loss_grad), that is the perceptron model is a 10-tuple containing the weight matrix W_ih, the bias vector b_ih, the weight vector W_ho and the bias scalar b_ho, the activation function for the units in the hidden layer hidden_activation_func, and its derivative hidden_activation_func_grad, the activation function for the unit in the output layer out_activation_func and its derivative out_activation_func_grad the loss function loss and the derivative of the loss function loss_grad. The suffix _ih stands for input to hidden, while _ho stands for hidden to output.\n",
    "\n",
    "a) perceptron_model = init_multilayer_perceptron(in_dim, hidden_dim, hidden_activation_func, hidden_activation_func_grad, out_activation_func, out_activation_func_grad, loss, loss_grad, init_size=1e-3) that takes in input a desired integer input dimension in_dim, a desired integer hidden_dim to specify the number of units in the hidden layer, the desired activation functions for the units in the hidden layer and the output unit, and loss functions and a desired float size init_size. This function initializes the perceptron model with random values. The weight matrices, vectors and scalars are initialised with values in the range (-init_size, +init_size). The weight matrix W_ih has dimensionality equal to in_dim x hidden_dim, the bias vector b_ih is a column vector of size hidden_dim, the weight vector W_ho has size hidden_dim and b_ho is a scalar.\n",
    "\n",
    "b) y or (y, a, h, h_ih)  = forward_multilayer_perceptron(x, perceptron_model, return_pre_activation=False) that takes in input a vector x and a model perceptron_model and returns the output prediction according to the perceptron algorithm. If the flag return_pre_activation is set to True then the output is a 4-tuple where the first element is the predicted output and the second element is the pre-activation value for the output unit, h is a vector containing the post-activations for the units in the hidden layer and h_ih is a vector containing the pre-activations for the units in the hidden layer. \n",
    "\n",
    "c) grad = compute_gradient_multilayer_perceptron(x, t, perceptron_model) that takes in input a vector x, a target value t, and a model perceptron_model and returns the gradients grad according to the perceptron algorithm. grad = (DW_ih, Db_ih, DW_ho, Db_ho) is a 4-tuple where the first element is the gradient of the weight matrix W_ih, the second element is the gradient of the vector b_ih, the third element is the gradient of the vector W_ho and the last element is the increment of the scalar b_ho. \n",
    "\n",
    "d) perceptron_model = update_multilayer_perceptron(grads, learning_rate, perceptron_model) that takes in input a list grads of gradients (each a 4-tuple), a learning rate float value of learning_rate and a model perceptron_model and returns an updated model according to the perceptron algorithm, that is, all the updates contained in grads are performed.\n",
    "\n",
    "e) perceptron_model = fit_multilayer_perceptron(X_train, y_train, hidden_activation_func, hidden_activation_func_grad, out_activation_func, out_activation_func_grad, loss, loss_grad, learning_rate, hidden_dim, batch_size=10, max_n_iter=1000, verbose=False) that takes in input a data matrix X_train a target vector y_train, the desired activation functions for the units in the hidden layer and in the output layer, and loss function, all with their respective derivative functions, a learning rate float value of learning_rate, an integer to define the number of units in the hidden layer hidden_dim, an integer to define the batch size batch_size, that is how many instances are processed before updating the model, and\n",
    "a maximum number of iterations of max_n_iter and returns a fit perceptron model according to the perceptron algorithm. If the flag verbose is set to True then every 100 iterations the loss is computed on the training data set and its value is printed to screen (this is useful to check that the loss is decreasing as training is progressing).\n",
    "\n",
    "f) scores = score_multilayer_perceptron(X_test, perceptron_model) that takes in input a data matrix X_test, a fit perceptron model perceptron_model and returns a score vector scores containing real values such that larger positive values indicate a preference for the positive class and viceversa for negative values. \n",
    "\n",
    "g) preds = predict_multilayer_perceptron(X_test, perceptron_model) that takes in input a data matrix X_test, a fit perceptron model perceptron_model and returns a prediction vector preds containing values 1 when predicting the positive class and 0 when predicting the negative class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b74662ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_multilayer_perceptron(in_dim, hidden_dim, hidden_activation_func, hidden_activation_func_grad, out_activation_func, out_activation_func_grad, loss, loss_grad, init_size=1e-3):\n",
    "    \n",
    "    b_ih=np.random.uniform(-init_size,init_size,(hidden_dim,1))\n",
    "    W_ih=np.random.uniform(-init_size,init_size,(in_dim,hidden_dim))   \n",
    "    b_ho=np.random.uniform(-init_size,init_size)\n",
    "    W_ho=np.random.uniform(-init_size,init_size,hidden_dim)\n",
    "    \n",
    "    model=(W_ih, b_ih, W_ho, b_ho, hidden_activation_func, hidden_activation_func_grad, out_activation_func, out_activation_func_grad, loss, loss_grad)\n",
    "    return model\n",
    "    \n",
    "def forward_multilayer_perceptron(x, perceptron_model, return_pre_activation=False):\n",
    "    W_ih, b_ih, W_ho, b_ho, hidden_activation_func, hidden_activation_func_grad, out_activation_func, out_activation_func_grad, loss, loss_grad=perceptron_model\n",
    "    b_ih=b_ih.T.flatten()\n",
    "    h_ih=np.dot(x,W_ih)+b_ih\n",
    "    h=hidden_activation_func(h_ih)\n",
    "    \n",
    "    a=np.dot(h,W_ho)+b_ho\n",
    "    y=out_activation_func(a)\n",
    "    \n",
    "    if return_pre_activation==False:\n",
    "        return y\n",
    "    return (y, a, h, h_ih)\n",
    "    \n",
    "def compute_gradient_multilayer_perceptron(x, t, perceptron_model):\n",
    "    W_ih, b_ih, W_ho, b_ho, hidden_activation_func, hidden_activation_func_grad, out_activation_func, out_activation_func_grad, loss, loss_grad=perceptron_model\n",
    "    y, a, h, h_ih=forward_multilayer_perceptron(x, perceptron_model, return_pre_activation=True)\n",
    "    error_grad=loss_grad(y,t)\n",
    "    derivative=out_activation_func_grad(y)\n",
    "    delta=derivative*error_grad\n",
    "    W_ho=delta*h\n",
    "    b_ho=delta    \n",
    "    grad_in=hidden_activation_func_grad(h)\n",
    "    delta1=grad_in*(W_ho*delta)\n",
    "    b_ih=delta\n",
    "    b_ih=np.reshape(b_ih,(-1,1)) \n",
    "    delta=np.reshape(delta,(1,-1))\n",
    "    x=np.reshape(x,(-1,1))\n",
    "    W_ih=x@delta    \n",
    "    return (W_ih, b_ih, W_ho, b_ho)\n",
    "    \n",
    "def update_multilayer_perceptron(grads, learning_rate, perceptron_model):\n",
    "    DW_ih, Db_ih, DW_ho, Db_ho=grads\n",
    "    \n",
    "    W_ih, b_ih, W_ho, b_ho, hidden_activation_func, hidden_activation_func_grad, out_activation_func, out_activation_func_grad, loss, loss_grad=perceptron_model \n",
    "    W_ih=W_ih+learning_rate*DW_ih\n",
    "    b_ih=b_ih+learning_rate*Db_ih\n",
    "    W_ho=W_ho+learning_rate*DW_ho\n",
    "    b_ho=b_ho+learning_rate*Db_ho\n",
    "    \n",
    "    model=(W_ih, b_ih, W_ho, b_ho, hidden_activation_func, hidden_activation_func_grad, out_activation_func, out_activation_func_grad, loss, loss_grad)\n",
    "    return model\n",
    "    \n",
    "def avg_grads(l,n):\n",
    "    init=l[0]\n",
    "    for i in range(1,n):\n",
    "        init+=l[i]\n",
    "    init=init/n\n",
    "    return init\n",
    "\n",
    "\n",
    "def fit_multilayer_perceptron(X_train, y_train, hidden_activation_func, hidden_activation_func_grad, out_activation_func, out_activation_func_grad, loss, loss_grad, learning_rate, hidden_dim, batch_size=10, max_n_iter=1000, verbose=False):\n",
    "    in_dim=np.shape(X_train)[1]\n",
    "    \n",
    "    model=init_multilayer_perceptron(in_dim, hidden_dim, hidden_activation_func, hidden_activation_func_grad, out_activation_func, out_activation_func_grad, loss, loss_grad, init_size=1e-3)\n",
    "    for j in range(max_n_iter):\n",
    "        if verbose==True and j%100==0:\n",
    "            preds=predict_multilayer_perceptron(X_train, model)\n",
    "            l=loss(preds,y_train)\n",
    "            print('Loss: ',l.mean())\n",
    "        W_in=[]\n",
    "        W_o=[]\n",
    "        b_in=[]\n",
    "        b_o=[]\n",
    "        for i in range(np.shape(X_train)[0]):\n",
    "            grads=compute_gradient_multilayer_perceptron(X_train[i], y_train[i], model)\n",
    "            W_in.append(grads[0])\n",
    "            b_in.append(grads[1])\n",
    "            W_o.append(grads[2])\n",
    "            b_o.append(grads[3])\n",
    "            if (i+1)%batch_size==0:\n",
    "                W_in=avg_grads(W_in,batch_size)\n",
    "                b_in=avg_grads(b_in,batch_size)\n",
    "                W_o=avg_grads(W_o,batch_size)\n",
    "                b_o=avg_grads(b_o,batch_size)\n",
    "                grads=(W_in,b_in,W_o,b_o)\n",
    "                mod=update_multilayer_perceptron(grads, learning_rate, model)\n",
    "                model=mod\n",
    "                W_in=[]\n",
    "                W_o=[]\n",
    "                b_in=[]\n",
    "                b_o=[]\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def score_multilayer_perceptron(X_test, perceptron_model):\n",
    "    scores=np.zeros(np.shape(X_test)[0],dtype=float)\n",
    "    for i in range(np.shape(X_test)[0]):\n",
    "        scores[i]=forward_multilayer_perceptron(X_test[i], perceptron_model,return_pre_activation=False)\n",
    "        \n",
    "    return scores\n",
    "    \n",
    "def predict_multilayer_perceptron(X_test, perceptron_model):\n",
    "    preds=np.zeros(np.shape(X_test)[0])\n",
    "    scores=score_multilayer_perceptron(X_test, perceptron_model)\n",
    "   \n",
    "    for i in range(len(scores)):\n",
    "        \n",
    "        if scores[i]>0:\n",
    "            preds[i]=1\n",
    "        else:\n",
    "            preds[i]=0\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d5918",
   "metadata": {},
   "source": [
    "Checkpoint to assess whether the functions were implemented correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cad865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-0804b03005a4>:32: RuntimeWarning: overflow encountered in exp\n",
      "  sigma=1/(1+np.exp(-x))\n",
      "<ipython-input-13-8181125eaae2>:33: RuntimeWarning: overflow encountered in multiply\n",
      "  delta1=grad_in*(W_ho*delta)\n",
      "<ipython-input-13-8181125eaae2>:17: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  a=np.dot(h,W_ho)+b_ho\n"
     ]
    }
   ],
   "source": [
    "# classification\n",
    "data, y, pos_landmarks, neg_landmarks = make_dataset_classification(size=300, complexity=2, ndim=2, return_landmarks=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=.3)\n",
    "X_test, y_test = make_2d_grid_dataset_classification(3000, pos_landmarks, neg_landmarks)\n",
    "\n",
    "perceptron_model = fit_multilayer_perceptron(X_train, y_train, \n",
    "                                             relu_activation, relu_activation_grad, \n",
    "                                             logistic_activation, logistic_activation_grad, \n",
    "                                             binary_crossentropy_loss, binary_crossentropy_loss_grad, \n",
    "                                             learning_rate=1e-2, hidden_dim=4, batch_size=10, \n",
    "                                             max_n_iter=1000, verbose=False)\n",
    "preds = predict_multilayer_perceptron(X_test, perceptron_model)\n",
    "plot_2d_classification(X_test, y_test, preds, pos_landmarks, neg_landmarks)\n",
    "\n",
    "# regression\n",
    "data, y, landmarks = make_dataset_regression(size=300, complexity=2, ndim=2, return_landmarks=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=.3)\n",
    "X_test, y_test = make_2d_grid_dataset_regression(3000, landmarks)\n",
    "\n",
    "perceptron_model = fit_multilayer_perceptron(X_train, y_train, \n",
    "                                             relu_activation, relu_activation_grad, \n",
    "                                             linear_activation, linear_activation_grad, \n",
    "                                             mse_loss, mse_loss_grad, \n",
    "                                             learning_rate=1e-2, hidden_dim=4, batch_size=10, \n",
    "                                             max_n_iter=1000, verbose=False)\n",
    "preds = score_multilayer_perceptron(X_test, perceptron_model)\n",
    "plot_2d_regression(X_test, y_test, preds, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94d8ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
